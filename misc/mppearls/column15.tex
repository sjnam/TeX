% More Programming Pearls
% Confessions of a Coder
% Jon Bentley
%
% Part IV: Algorithms
% Column 15: Selection

\input luamplib.sty
\input mppearls

% Token for \aftergroup to reset color
\def\colorstackpop{\pdfextension colorstack 0 pop}
%\def\setcmykcolor#1{%
%  \pdfextension colorstack 0 push {#1 k #1 K}%
%  \aftergroup\colorstackpop
%}
\def\setrgbcolor#1{%
  \pdfextension colorstack 0 push {#1 rg #1 RG}%
  \aftergroup\colorstackpop
}
%\def\black{\setcmykcolor{0 0 0 1}}
%\def\blue{\setcmykcolor{1 1 0 0}}
\def\red{\setrgbcolor{0.8 0 0}}

\pageno=159
\columntitle 15:selection

Suppose you have a list of heights of 101 people. It isn't too hard to find the
tallest or the shortest on the list, but how would you identify the most
mediocre person (speaking only heightwise, of course)? That is, how would you
find the person on the list who is taller than the fifty shortest people and
shorter than the fifty tallest?

The next section describes the problem around which this is built: selecting the
$K^{\rm th}$-smallest member in the set of $N$ elements. A program for the taks
is derived in the following section, and the subsequent section analyzes its
rapid running time.

\section The Problem

This excerpt from the entitled ``Density of Population by States'' gives the
1980 figures in persons per square mile.
$$
\vbox{\offinterlineskip
\halign{&\vrule#&\strut#\hfil\quad\cr
\multispan5\hrulefill\cr
\noalign{\vskip1pt}
\multispan5\hrulefill\cr
width0pt height3pt&\omit&&\omit&\omit\cr
\omit&\hfil\vbox to6mm{\vfil\hbox to.8in{\hfil\sc Name\hfil}\vfil}&&
  \hskip16pt\vbox to6mm{\vss\hbox to14mm{\hss\sc Population\hss}\par\vskip4pt
  \hbox to14mm{\hfil\sc Density\hfil}\vss}&\omit\cr
width0pt height2pt&\omit&&\omit&\omit\cr
\multispan5\hrulefill\cr
width0pt height2pt&\omit&&\omit&\omit\cr
\omit&West Virginia&&\hfill80.8&\omit\cr
\omit&North Carolina&&\hfill120.4&\omit\cr
\omit&Virginia&&\hfill134.7&\omit\cr
\omit&Pennsylvania&&\hfill264.3&\omit\cr
\omit&New York&&\hfill370.6&\omit\cr
\omit&Maryland&&\hfill428.7&\omit\cr
\omit&Connecticut&&\hfill637.8&\omit\cr
\omit&New Jersey&&\hfill986.2&\omit\cr
\omit&District of Columbia&&\hfill10,132.3&\omit\cr
width0pt height2pt&\omit&&\omit&\omit\cr
\multispan5\hrulefill\cr}}$$
If you had to choose a single number to characterize the ``typical'' density in
these nine contiguous areas, what would it be? The average or arithmetic mean
value is 1461.8, but that seems to high: it is greater than eight of the nine
values. New York's ``middle'' value of 370.6 seems more representative; it is
the fifth largest of the nine. Statisticians refer to the $M+1^{\rm st}$-smallest
element is a set of $2M+1$ elements as the {\it median\/}, or its $50^{\rm th}$
percentile. We'll use medians and other percentiles later in this column to
analyze data on the run time of the selection algorithm.

Computer scientists use medians in many ``divide-and-conquer'' algorithms. The
median partitions a set into two subsets which an algorithm then processes
recursively; Problem 8 uses an algorithm with this structure. Furthermore, the
selection problem is a practical introduction to the theoretical field of
comparison algorithms; Problem 9 presents two other representative problems.

Let's turn now from the abstract world sets to the concrete world of programs.
The input to our selection routine will be the positive integer $n,$ the array
$x[1\ltdot n],$ and the positive integer $k\le n.$ The program must permute the
array so that $x[1\ltdot k-1]\le x[k]\le x[k+1\ltdot n].$ At that point, the
$k^{\rm th}$-smallest element in the set resides in its proper position, $x[k].$

\section The Program

A simple selection program merely sorts the array $x.$ Unfortunately, this
straightforward solution requires $O(n\log n)$ time. In this section we'll study
a faster algorithm due to \hbox{C. A. R. Hoare}. His method selects the $k^{\rm
th}$-smallest element in just $O(n)$ average time. Hoare called his program {\it
find\/}; I'll refer to this implementation in this column as {\it select.}

Hoare's selection algorithm is closely related to his Quicksort program. That
divide-and-conquer algorithm can be sketched as
\begindisplay
\vbox{
\+\bf procedure ${\it qsort}({\bf set}\ S):$ sequence\cr
\+\quad&\bf if ${\it size}(S) \le1$ then\cr
\+&\quad&{\bf return} the element in $S$\cr
\+&\bf else\cr
\+&&partition $S$ around a random slement\cr
\+&&\quad&$t$ into subsets $a$ and $b$ such that\cr
\+&&&elements in $a$ are less than $t$ and\cr
\+&&&elements in $b$ are greater than $t$\cr
\+&&{\bf return} ${\it qsort}(a)$ followed by $t$\cr
\+&&&followed by ${\it qsort}(b)$\cr}
\enddisplay
The procedure's input in a set and its output is the sequences of elements in
sorted order. Both input and output structures can be efficiently implemented
in a single array: the sequence is the subvector $x[l\ltdot u]$ is rpresented
by the two integers $l$ and $u.$

The {\it select} algorithm has the same structure as Quicksort. Given $l\le
k\le u,$ its first step in finding the proper occupant of $x[k]$ is to
partition the array around a random element. While Quicksort then recursively
operate on both subsequences, {\it select} saves time by recurring only on the
side that contains $k.$ Here is Slect as it finds the medians of a 21-element
array:
$$
\centerline{\vbox{\baselineskip=16pt
\halign{&\hbox to15pt{\hfil\strut#}\cr
[21&5&15&7&19&7&{\red\bf22}&75&65&39&25&73&98&95&53&39&27&63&46&58&82\rlap{]}\cr
&&&&&&&[27&25&{\red\bf39}&65&73&98&95&53&75&39&63&46&58&82\rlap{]}\cr
&&&&&&&&&&[58&73&53&65&39&63&46&{\red\bf75}&98&95&82\rlap{]}\cr
&&&&&&&&&&[46&39&{\red\bf53}&65&73&63&58\rlap{]}&&&&\cr
&&&&&&&&&&[39&{\red\bf46}\rlap{]}&&&&&&&&&\cr
\noalign{\medskip}
[21&5&15&7&19&7&22&27&25&39&{\red\bf39}&46&53&65&73&63&58&75&98&95&82\rlap{]}\cr
}}}
$$
Each level in the picture represents a stage of the algorithm, and the array's
final configuration is described in the last level. The partitioning element
is red colored. Elements to its left have lesser values, while elements to its
right are greater than or equal to the partitioning value.

An iterative selection algorithm can be sketched as follows.
\begindisplay
\vbox{
\+set range to entire array\cr
\+{\bf while} range is large {\bf do}\cr
\+\quad partition range\cr
\+\quad repeat on proper subrange\cr}
\enddisplay
We'll first study the partitioning code, and then turn to the complete the
algorithm.

The routine partitions the array $x[l\ltdot u]$ around the value $t=x[l].$
After the $i-1^{st}$ step of the iteration, the loop invariant is depicted as
$$
\mplibcode
input boxes
beginfig(0);
boxjoin(a.se=b.sw; a.ne=b.nw);
boxit.a(btex\strut$t$etex);
boxit.b(btex\strut\hbox to1in{\strut\hfil${}<t$\hfil}etex);
boxit.c(btex\strut\hbox to1in{\strut\hfil${}\ge t$\hfil}etex);
boxit.d(btex\strut\hbox to.8in{\strut\hfil?\hfil}etex);
drawboxed(a,b,c,d);
pair L,M,I,U;
L=a.c-(1mm,7mm);
label.bot(btex $x$ etex, L);
drawarrow L -- (a.s-(1mm,0));
M=b.e-(2mm,7mm);
label.bot(btex $m$ etex, M);
drawarrow M -- (b.se-(2mm,0));
I=d.w+(2mm,-7mm);
label.bot(btex $i$ etex, I);
drawarrow I -- (d.sw+(2mm,0));
U=d.w+(21mm,-7mm);
label.bot(btex $u$ etex, U);
drawarrow U -- (d.sw+(21mm,0));
endfig;
\endmplibcode
$$
The iterative step inspects the $i^{\rm th}$ element. If $x[i]\ge t$ then the
invariant remains true. When $x[i]<t,$ we regain the invariant by incrementing
$m$ to index the new location of the small element, and then swapping $x[m]$
with $x[i].$ The loop terminates with $i=u+1,$ leaving
$$
\mplibcode
input boxes
beginfig(0);
boxjoin(a.se=b.sw; a.ne=b.nw);
boxit.a(btex\strut$t$etex);
boxit.b(btex\strut\hbox to 1.4in{\strut\hfil${}<t$\hfil}etex);
boxit.c(btex\strut\hbox to 1.4in{\strut\hfil${}\ge t$\hfil}etex);
drawboxed(a,b,c);
pair L,M,U;
L=a.c-(1mm,7mm);
label.bot(btex $l$ etex, L);
drawarrow L -- (a.s-(1mm,0));
M=b.e-(2mm,7mm);
label.bot(btex $m$ etex, M);
drawarrow M -- (b.se-(2mm,0));
U=c.w+(36mm,-7mm);
label.bot(btex $u$ etex, U);
drawarrow U -- (c.sw+(36mm,0));
endfig;
\endmplibcode
$$
We then swap $x[l]$ with $x[m]$ to give
$$
\mplibcode
input boxes
beginfig(0);
boxjoin(a.se=b.sw; a.ne=b.nw);
boxit.b(btex\strut\hbox to 1.4in{\strut\hfil${}<t$\hfil}etex);
boxit.a(btex\strut$t$etex);
boxit.c(btex\strut\hbox to 1.4in{\strut\hfil${}\ge t$\hfil}etex);
drawboxed(b,a,c);
pair L,M,U;
L=b.c-(17mm,7mm);
label.bot(btex $l$ etex, L);
drawarrow L -- (b.s-(17mm,0));
M=a.e-(2mm,7mm);
label.bot(btex $m$ etex, M);
drawarrow M -- (a.se-(2mm,0));
U=c.w+(36mm,-7mm);
label.bot(btex $u$ etex, U);
drawarrow U -- (c.sw+(36mm,0));
endfig;
\endmplibcode
$$
That final swap ensures that we can operate next on $l\ltdot m-1$ or $m+1\ltdot
u.$ In both cases, we exclude $x[m],$ and thereby avoid an infinite loop.

Partitioning around the first element in the array can require excessive time
for some common inputs --- for instance, arrays that are already sorted. We do
better to choose a partitioning element at random. We'll accomplish this by
swapping $x[l]$ with a random entry in $x[l\ltdot u],$ using the function
${\it rand\_int}(l,u)$ described in Section 13.1 which returns a random integer
in the range $l\ltdot u.$ The complete partioning code is
\begindisplay
\vbox{
\+${\it swap}(x[l], x[{\it rand\_int}(l,u)])$\cr
\+$m\leftarrow l$\cr
\+\bf for $i\leftarrow l+1$ {\rm to} $u$ do\cr
\+\quad\bf if $x[i]<x[l]$ then\cr
\+\qquad$m\leftarrow m+1$\cr
\+\qquad${\it swap}(x[m],x[i])$\cr
\+${\it swap}(x[l],x[m])$\cr}
\enddisplay
Upon termination, we know that $x[l\ltdot m-1]<x[m]\le x[m+1\ltdot u].$

With this partitioning code in hand, we can turn our attention to the complete
selection subroutine. Our first version is recursive: $select(l,u,k)$ partitions
the array $x[l\ltdot u]$ so $x[l\ltdot k-1]<x[k]\le x[k+1\ltdot u].$ If $l\ge
u$ then the subarray contains at most one element, so we can halt. Otherwise, we
partition the array around the element $t,$ which is placed in $x[m].$ The
position of $k$ relative to $m$ gives three cases:
$$
\mplibcode
input boxes
beginfig(0);
boxjoin(a.se=b.sw; a.ne=b.nw);
boxit.b(btex\strut\hbox to 1.4in{\strut\hfil${}<t$\hfil}etex);
boxit.a(btex\strut$t$etex);
boxit.c(btex\strut\hbox to 1.4in{\strut\hfil${}\ge t$\hfil}etex);
drawboxed(b,a,c);
pair L,M,U,CI,CII,CIII;
L=b.c-(17mm,7mm);
label.bot(btex $l$ etex, L);
drawarrow L -- (b.s-(17mm,0));
M=a.e-(2mm,7mm);
label.bot(btex $m$ etex, M);
drawarrow M -- (a.se-(2mm,0));
U=c.w+(36mm,-7mm);
label.bot(btex $u$ etex, U);
drawarrow U -- (c.sw+(36mm,0));
CI=b.n+(0,4mm);
label.top(btex Case 1 etex, CI);
drawarrow CI -- b.n;
CII=a.n+(0,4mm);
label.top(btex Case 2 etex,CII);
drawarrow CII -- a.n;
CIII=c.n+(0,4mm);
label.top(btex Case 3 etex,CIII);
drawarrow CIII -- c.n;
endfig;
\endmplibcode
$$
Case 2 is the easist. When $k=m,$ the $k^{\rm th}$-smallest element is in its
final place and the program is finished. When $k<m$ we have Case 1: the $k^
{\rm th}$-smallest element can't be in $x[m\ltdot u],$ so we exclude that range
by recursively operation on the range $l\ltdot m-1.$ Case 3 is similar, and
the recursive routine can be sketched as
\begindisplay
\vbox{
\+\bf procedure ${\it select}(l,u,k)$\cr
\+\qquad\hbox to 10mm{{\bf pre}:\hfil}$l\le k\le u$\cr
\+\qquad\hbox to 10mm{{\bf post}:\hfil}$x[l\ltdot k-1]<x[k]\le x[k+1\ltdot u]$
\cr
\+\quad\bf if $l<u$ then\cr
\+\qquad\tt /* Partition x[l..u] so that \cr
\+\qquad\quad\tt\ x[l..m-1] <= x[m] <= x[m+1..u] */\cr
\+\qquad\hbox to 12mm{\bf if\hfil}$k<m$ \bf then ${\it select}(l,m-1,k)$\cr
\+\qquad\hbox to 12mm{\bf else if\hfil}$k>m$ \bf then ${\it select}(m+1,u,k)$\cr
\+\qquad\tt /* else k = m so finished */\cr}
\enddisplay
Since $x[m]$ is excluded by each recursive cll, the program can't have an
infinite loop.

The recursive calls in the above precedure are of a special form called {\it
tail recursion\/}: the call is always the last action in a precedure. A tail-%
recursive procedure can always be transformed into an equivalent precedure with
a {\bf while} loop. We'll now study an iterative selection subroutine, which we
saw earlier in Section 3.2. It uses $l$ and $u$ as local variables, maintaining
the relation that $l\le k\le u$ until the final step. After partitioning around
$x[m],$ the code adjusts $l$ or $u$ (and sometimes both) to narrow thr range
$l\ltdot u.$ Here is the final version of {\it select\/} program:
\begindisplay
\vbox{
\+\bf procedure ${\it select}(k)$\cr
\+\qquad\hbox to 10mm{{\bf pre}:\hfil}$1\le k\le n$\cr
\+\qquad\hbox to 10mm{{\bf post}:\hfil}$x[1\ltdot k-1]<x[k]\le x[k+1\ltdot n]$
\cr
\+\quad$l\leftarrow1;\ u\leftarrow n$\cr
\+\quad\bf while $l<u$ do\cr
\+\qquad\tt /* Invariant:$\;$x[1..l-1] <= x[l..u] <= x[u+1..n] */\cr
\+\qquad${\it swap}(x[l],x[{\it rand\_int}(l,u)])$\cr
\+\qquad$m\leftarrow l$\cr
\+\qquad\bf for $i\leftarrow l+1$ {\rm to} $u$ do\cr
\+\qquad\quad\tt /* Invariant:$\;$&\tt x[l+1..m] < x[l] and\cr
\+&\tt x[m+1..i-1] >= x[l] */\cr
\+\qquad\quad\bf if $x[i]<x[l]$ then\cr
\+\qquad\qquad$m\leftarrow m+1$\cr
\+\qquad\qquad${\it swap}(x[m],x[i])$\cr
\+\qquad${\it swap}(x[l],x[m])$\cr
\+\qquad\tt /* \cleartabs&\tt x[1..l-1] <= x[l..u] <= x[u+1..n]\cr
\+&\tt and x[l..m-1] <= x[m] <= x[m+1..u] */\cr
\+\qquad\bf if $k\le m$ then $u\leftarrow m-1$\cr
\+\qquad\bf if $k\ge m$ then $l\leftarrow m+1$\cr}
\enddisplay
This is the {\it select\/} algorithm we'll study in the rest of this column, and
it is fine for typical day-to-day use. There are, however, several improvements
that one should incorporate into an industrial-strength selection routine.
Speedups to the partitioning code are described in Problem 1, 2, 4 and 5.

\section Analysis of Run Time

In the previous section we derived a selection routine and informally analyzed
its correctness: it halts on all inputs, and always computes the correct answer.
We'll turn now to its allegedly linear run time. The intuitive idea behind the
$O(n)$ average time is that typical iterations remove a substantial fraction
of the range $L\ltdot U.$ If each step were to remove half the elements, then
an identity like
$$
n+n/2+n/4+n/8+\cdots\le 2n
$$
would describe the total run time.

This section supports our intuition with observations of the algorithm at work.
In addition to insight about {\it select\/}, this exercise illustrates general
techniques for the empirical analysis of algorithms. (Problem 6 introduces the
mathematical analysis of section algorithms.)

The first figure in Section 15.2 illustrates the algorithm's behavior on an
array of 21 elements. That figure is useful as one first studies the algorithm,
but it is too detailed to give much insight into the algorithm's performance.
Here is a similar picture of an array, and a ``stick diagram'' representation of
the computation:

To estimate the constant $C_{\rm median}$, we'll gather data on the number of
comparisons used by the algorithm. Instead of running the complete algorithm
on real data, we'll use this ``skeleton'' program to count comparisons.
\begindisplay
\vbox{
\+$ccount\leftarrow0$\cr
\+$l\leftarrow1;\ u\leftarrow n$\cr
\+\bf while $l<u$ do\cr
\+\quad$ccount\leftarrow ccount+u-l$\cr
\+\quad$m\leftarrow {\it rand\_int}(l,u)$\cr
\+\quad\bf if $k\le m$ then $u\leftarrow m-1$\cr
\+\quad\bf if $k\ge m$ then $l\leftarrow m+1$\cr}
\enddisplay
The {\it select\/} program uses $u-l$ comparisons to partition the $u-l+1$
elements in the range $l\ltdot u.$ The above program can simulate the
computation on a set of size $n=10^6$ in a few dozen steps rather than a few
million steps.

So far our analysis of {\it select\/} has concentrated on the fact that it uses
$O(n)$ comparisons. Because it does only some constant number of other
operations alone with each comparison, its total running time is also linear. To
gain further insight I implemented {\it select\/} in C on VAX-11/750 and
compared it to the C library subroutine {\it qsort\/}. The system sort required
about $100n\log_2n$ microseconds to sort an array of $n$ elements, while {\it
select\/} found the median in about $100n$ microseconds. For $n=100{,}000$, this
translates into ten seconds for {\it select\/} versus almost three minutes for
the sort.

\section Principles

We have analyzed two aspects of Hoare's selection algorithm: its answer is {\it
correct\/}, and it computes that answer {\it efficiently\/}. This exercise
illustrates two important points about analysis of programs.

{\it A Spectrum of Analyses}. There are sevral reasons why I believe that the
{\it select} program is correct. This column presented both an informal
correctness argument and pictures showing the algorithm at work (generated by
the program itself). Section 3.2 describes scaffolding for viewing the program
at work and for testing the program. Each of these analyses supports the
others: watching the program at work gives insight into its loop invariant,
which in turn is useful for testing.

I am also convinced that {\it select} runs in $O(n)$ time on arrays with few
duplicated elements. This column supports that premise with an informal
mathematical argument (the ``halving model'') and a series of experiments
observing the program at work. The experiments progressed from detailed pictures
of the array to ``sick diagrams'' illustrating the size of the subrange to
graphs counting the number of comparisons. Each experiment in the series
described more computations but gave less information about each one. Problem 6
continues this trend, and shows how abstraction of the program can eventually
lead to a mathematical analysis.

{\it Skeleton Programs}. We saw several programs that provide information about
{\it select} without performing all the work of the complete program. Problem 6
describes several additional programs with this flavor. While {\it select}
would use sevral billion stpes on a set of size one billion, these programs can
gather information on the same computation in just a few dozen operations. These
programs represent important midpoints on the pectrum of analyses sketched
above.

{\it Graphical Methods in Analysis}. Graphical output is now available to many
programmers; we should use it to understand our programs. All pictures in this
column were drawn by simple programs (between ten and thirty line of code). We
understood the correctness of the algorithm with detailed pictures showing the
history of the computations and ``array boxes'' that illustrated the loop
invariants. Graphical displays allow us to analyze a large volumn of
experimental data. The right graph in the last figure, for instance, uses about
150 horizontal and vertical line segments to represent 550 computations that
together represent over a billion comparisons. The mathematical analysis of most
algorithm is downright hard, but simulations and pictures are well within the
grasp of most programmers.

\section Problems

\vskip-\medskipamount

\prob
{\it select} partitions about a random element in the subrange. Study the
effectiveness of using other partitioning elements (such as the median of the
first, middle and last elements in the array or an appropriate representative
of a large sample).

\prob
The {\it select} algorithm and its derivatives aren't always the best way to
implement selection. How would you select the second-smallest element in a
three-element array? What if $k=6$ and $n=11?$ What if $k=1000$ and the $n=
1000{,}000$ input values were stored on a reel of magnetic tape?

\prob
How would you find the median of one millian values stored on magnetic tape if
your computer had only one tape drive and about a dozen words of main memory?
How would you use a second tape drive?

\prob
Although {\it select} runs in $O(n)$ average time, it requires $O(n^2)$ time in
the worst case. Describe a selection algorithm with $O(n)$ worst-case time.

\prob
Perform experiments and display data for the following problems.

\pitm{a.} The discussion of run time concentrated on the number of comparisons
used; that is a good but sometimes imperfect indicator of cost on a real
machine. Implement a selection algorithm and measure its run time. Any
surprises?

\pitm{b.} Delete randomizing {\it swap\/} statement from the {\it select}
program. How does the average run time change? Describe an input that achieves
the worst possible run time.

\pitm{c.} The first graph in Section 15.3 held $k$ fixed at $(n+1)/2$ and varied
$n,$ and the next graph held $n$ fixed at 1,000,001 and varied $k.$ Describe
the function of two variables that tells the average number of comparisons
needed to find the $k^{\rm th}$-smallest element in a set of $n$ distinct
elements. In particular, what is the shape of the curve induced by varying $k$
when $n$ is fixed? When $k$ is fixed at a constant fraction of $n,$ how does
that curve behave?

\pitm{d.} Our analyses assumed that the input array contained no duplicated
elements. How does {\it select} perform if some array elements appear many
times? How can that performance be improved?

\prob
This problem mathematically investicates the performance of {\it select} when
it is called with $k=1,$ that is, when it selects the least element in the
array. The skeleton program that counts comparisons (without actually selecting
the least element) simplifies to
\begindisplay
\vbox{
\+$u\leftarrow n$\cr
\+\bf while $u>1$ do\cr
\+\quad$ccount\leftarrow ccount+u+1$\cr
\+\quad$u\leftarrow {\it rand\_int}(1,u)-1$\cr}
\enddisplay

\more
Show that this recursive program computes the same function
\begindisplay
\vbox{
\+\bf function ${\it ccount}(n)$\cr
\+\quad\bf if $n\le1$ then\cr
\+\qquad\bf return $0$\cr
\+\quad\bf else\cr
\+\qquad\bf return $n+1+{\it ccount}({\it rand\_int}(0,n-1))$\cr}
\enddisplay

\more
If $C_n$ denote the average value of ${\it ccount}(n)$ after the code is
executed, show that it satisfies the recurrence relation
$$\displaylines{
C_0=C_1=0\cr
C_n=n+1+1/n\sum_{0\le i\le n-1}C_i\cr}
$$
Write a program that computes $C_0,C_1,\ldots,C_m.$ (Hint: first use a table
$C[0\ltdot m]$ and $O(m^2)$ time, then make your algorithm run in $O(m)$ time,
and finally remove table.) Use that program to characterize the behavior of
$C_n.$ One possible use is to run the program to gather data, while another
approach studies its structure to see how to ``telescope'' the recurrence
analytically.

\prob
[J. M. Chambers] The {\it select} algorithm ensures that for a single value of
$k$, $x[1\ltdot k-1]\le x[k]\le x[k+1\ltdot n],$ while Quicksort establishes
that condition for all variable of $K.$ The problem of ``Partial Sorting''
calls for establishing the condition for a set of integers in the range $1
\ltdot n.$ For instance, in drawing box plots of 101 values we were intersted
in the $\{6,26,51,76,96\}.$ Show how to modify the Quicksort/Select idea to
compute partial sorts. Given the input arrays $1\le k[1]\le k[2]\le\cdots\le
k[m]\le n$ and $x[1\ltdot n],$ the program should establish
$$
\eqalign{
x[1\ltdot k[1]-1]&\le x[k[1]]\le \cr
x[k[1]+1\ltdot k[2]-1]&\le x[k[2]]\le \cr
x[k[2]+1\ltdot k[3]-1]&\le x[k[3]]\le\cdots \cr}
$$

\prob
For this problem, assume that every element of the array $x$ has two fields:
$x[i].{\it key}$ is the key of the $i^{\rm th}$ element, and $x[i].{\it wt}$ is
its weight (a positive real number). Let $x$ denote $\sum_{1\le i\le n}x[i].wt.$
The ``weighted median'' problem calls for computing the integer $k$ and
partitioning the array such that these conditions hold:
$$
\eqalign{
x[1\ltdot k-1].{\it key}\le&x[k].{\it key}\le x[k+1\ltdot n].{\it key}\cr
\sum_{1\le i<k}&x[i].wt < s/2\cr
\sum_{k<i\le n}&x[i].wt < s/2\cr}
$$
Modify {\it select} to perform this task in linear expected time. Show how to
use a solution to Problem 4 as a subroutine to solve this problem in linear
worst-case time. Modify both algorithms to find other ``weighted quantiles'':
given aa real $0<q<1,$ find a record such that the weights of lesser keys sum
to at most $qs,$ while the weights of greater keys sum to at most $(1-q)s.$

\prob
Give algorithms for finding both the minimun and maximum elements in a set and
for finding the maximum and second-largest elements. Try to use as few
comparisons as possible.

\prob
Experiment with other graphical representations of computations. This picture,
for instance, illlustrates the computation depicted in the final figures in
Section 15.2. Numbers in that figure are represented here as vertical bars.
Try ``animiating'' such a representation in a simple movie.

\section Further Reading

Hoare originally described Quicksort and Find in one page of the July 1961 {\sl
Communications of the ACM\/.} He illustrated the young field of program
verification by arguing the correctness of Find in the January 1971 {\sl
Communications\/.} Knuth analyzed the run time of the algorithm in his
``Mathematical Analysis of Algorithms'' on pages 19--27 of the proceedings of
the 1971 IFIP Congress. In the March 1975 {\sl Communications\/}, Floyd and
Rivest present a selection algorithm that use just $n+k+O(\sqrt n)$ comparisons.
Their algorithm is close to the theoretical optimum, and their code runs like
the wind.

\bye
