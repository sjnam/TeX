% More Programming Pearls
% Confessions of a Coder
% Jon Bentley
%
% Part IV: Algorithms
% Column 13: A Sample of Brilliance

\input pearls

\pageno=139
\column 13:A Sample of Brilliance

How can a computer deal a poker hand? If we assign each card in the deck its own
integer between $1$ and $52,$ then we can make a hand from a ``random sample''%
\thinspace of $5$ integers in the range $1\ltdot52,$ for instance,
\begindisplay
$4\ \;8\ \;31\ \;46\ \;47$
\enddisplay
(It is important that no number appear twice; I understand that holding more
than one ace of spades can seriously jeopardize a card player's health.) Random
samples also arise in applications as diverse as simulation, program testing,
and statistics.

The first section of this column reviews several standard algorithms for random
sampling. The next section describes an elegant new algorithm by Bob Floyd.
(when this column first appeared in {\sl Communications of the ACM,\/} Floyd's
name was on the byline as the ``Special Guest Oyster''). The third section then
describes how Floyd extends his algorithm to generate random permutations of
integers.

\section A Sampling of Sampling Algorithms

Before we can generate a random sample, we have to be able to generate a single
random number. We will therefore assume that we have a function ${\it
rand\_int}(l,u)$ that returns an integer uniformly distributed over $l\ltdot
u.$ {\eightpoint\parindent=8pt\footnote\dag{If you don't have a {\it
rand\_int\/} function, you can make one using a function {\it rand\/} that
returns a random real distributed uniformly in $[\,0,1)$ by the expression
$l+{\bf int}\,\bigl(rand\times(u+1-l)\bigr).$ In the unlikely event that your
system doesn't even have that routine, consult Knuth's {\sl Seminumerical
Algorithms.\/} But whether you use a system routine or make your own, be
careful that {\it rand\_int\/} returns a value in the range $l\ltdot u$ ---
a value out of range is nasty bug.}}

It is easy to generate a random sequence of $m$ integers in the range
$1\ltdot n,$ so long as we don't mind duplicates:
\begindisplay
\vbox{\+\bf for $i\leftarrow1$ to $m$ do\cr
\+\quad print ${\it rand\_int}(1,n)$\cr}
\enddisplay
When I invoked that program with $m$ set to $12$ and $n$ set to $3$, the code
produced the sequence
\begindisplay
$3\ \;1 \ \;3\ \;3\ \;1\ \;1\ \;1\ \;2\ \;1\ \;1\ \;3\ \;1$
\enddisplay
This very sequence might come in handy for your next tough game of
rock-paper-scissors. More serious applications include testing finite state
machines and testing sorting programs (see Section~3.3).

Many applications, though, require a random sample without ducplicates. A
statistical anlaysis, for instance, might waste work by observing the same
element twice. Such samples are often referred to as ``samples without
replacement'' or as ``combinations''. For the remainder of this column, though,
the word ``sample'' will denote a random sample with no duplicates. Solution~3
in Section~5.2 describes an application of a program like this.

Many sampling algorithms are based on this pseudocode, which we'll call
\alg S:
\begindisplay
\vbox{
\+$S\leftarrow\{\}$\cr
\+$size\leftarrow0$\cr
\+\bf while $size<m$ do\cr
\+\quad&$t\leftarrow{\it rand\_int}(1,n)$\cr
\+&\bf if $t\not\in S$ then\cr
\+&\quad& insert $t$ in $S$\cr
\+&&$size\leftarrow size+1$\cr}
\enddisplay
The algorithm stores the sample in the set $S$. If $S$ is implemented correctly
and if {\it rand\_int\/} produces random integers, then the algorithm produces a
random sample. That is, each $m$-element subset is produced with probability
$1/{n\choose m}.$ The loop invariant is that $S$ always contains a random sample
of $size$ integers in the range $1\ltdot n.$

There are four operations on the set $S$: initializing it to empty, testing an
integer for membership, inserting a new integer, and printing all the members.
{\sc column 11} of my 1986 book {\sl Programming Pearls\/} sketches the
algorithm and five data structures that can be used to implement the set $S$:
bit vectors, unsorted arrays, sorted arrays, binary search tree, and bins. It
also sketches several other algorithms for sampling; see Problem~9.

\section Floyd's Algorithm

\alg S\ has many virtues: it is correct, fairly efficient, and remarkably
succinct. It is so good, as a matter of fact, that I thought one couldn't do
better. I therefore charged ahead and described it in detail in a column.

Unfortunately, I was wrong; fourtunately, Bob Floyd caught me sleeping. When he
studied \alg S, he was bothered by a flaw that is displayed clearly when
$m=n=100.$ When $size=99,$ the set $S$ contains all but one of the desired
integers. The algorithm closes its eyes and blindly guesses integers until it
stumbles over the right one, which requires $100$ random numbers on the average.
That analysis assumes that the random nubmer generator is truly random. For some
nonrandom sequences, the algorithm won't even terminate.

Floyd set out to find an algorithm that uses exactly one call {\it rand\_int\/}
for each random number in $S$. The structure of Floyd's algorithm is easy to
understand recursively: to generate $5$-element sample from $1\ltdot10$, we
first generate a $4$-element sample from $1\ltdot9$, and then add the fifth
element. The recursive algorithm is sketched as \alg{F1}:
\begindisplay
\vbox{
\+\bf function ${\it sample}(m,n)$\cr
\+\quad& \bf if $m=0$ then\cr
\+&\quad&{\bf return} $\{\}$\cr
\+&\bf else\cr
\+&& $S\leftarrow{\it sample}(m-1,n-1)$\cr
\+&& $t\leftarrow{\it rand\_int}(1,n)$\cr
\+&&\bf if $t \not\in S$ then\cr
\+&&\quad&insert $t$ in $S$\cr
\+&&\bf else\cr
\+&&& insert $n$ in $S$\cr
\+&&\bf return $S$\cr}
\enddisplay
We can appreciate the correctness of \alg{F1} anecdotally. When $m=5$
and~$n=10$, the algorithm first recursively computes in $S$ a $4$-element
random sample from $1\ltdot9$. Next it assigns to $t$ a random integer in the
range $1\ltdot10.$ Of the $10$ values that $t$ can assume, exactly $5$ result
in inserting $10$ into $S$: the four values already in $S$, and the value $10$
itself. Thus element $10$ is inserted into the set with the correct probability
of $5/10$. The next section proves that \alg{F1} produces each $m$-element
sample of an $n$-set with equal probability.

Because \alg{F1} uses a restricted form of recursion, Floyd was able to
translate it to an iterative form by introducing a new variable $j$.
(Problem 8 and~Section 3.2 discuss the problem of recursion removal in more
general terms.) The result is \alg{F2}, witch is more efficient than \alg S\
yet almost as succinct:
\begindisplay
\vbox{
\+$S\leftarrow\{\}$\cr
\+\bf for $j\leftarrow n-m+1$ to $n$ do\cr
\+\quad&$t\leftarrow{\it rand\_int}(1,j)$\cr
\+&\bf if $t\not\in S$ then\cr
\+&\quad&insert $t$ in $S$\cr
\+&\bf else\cr
\+&& insert $j$ in $S$\cr}
\enddisplay
Problem 2 and~3 address the data structures that mgith be used to implement the
set $S$.

For thoes who might scoff that \alg{F2} is ``just pseudocode'', the next program
implements Floyd's algorithm in the Awk language. The associative array
described in {\sc column 2} provide a clean implementation of the set $S$. Awk's
{\tt ARGV} array allows the program to access command line arguments, so a
sample of $200$ elements in the range $1\ltdot1000$ can be generated by typing
\hbox{\tt sample 200 1000}. Complete with input and output, the Awk program
requires only eight lines:
\begindisplay
\vbox{
\+\tt BEGIN $\{$ &$m={\tt ARGV}[1]$; $n={\tt ARGV}[2]$\cr
\+&{\tt for} &$(\,j=n-m+1$; $j\le n$; $j\PP)\ \{$\cr
\+&&$t=1+{\tt int}(j*{\it rand}())$\cr
\+&&{\tt if} $(t\ {\tt in}\ s)\ s[j] = 1$\cr
\+&&{\tt else} $s[t] = 1$\cr
\+&$\}$\cr
\+&\tt for $(i\ {\tt in}\ s)$ {\it print} $i$\cr
\+$\}$\cr}
\enddisplay

\section Random Permutations

Some programs that use a random sample require that the elements of the sample
occur in a random order. Such a sequence is called a random permutation without
replacement. In testing a sorting program, for instance, it is important that
randomly generated input occur in random order; if the input were always sorted,
the test mgith not fully exercise the sort code.

We could use Floyd's \alg{F2} to generated a random sample, then copy it
to an array, and finally shuffle the elements of the array. This code randomly
scrambles the array $x[1\ltdot m]$:
\begindisplay
\vbox{
\+\bf for $i\leftarrow m$ downto $2$ do\cr
\+\quad&$j\leftarrow{\it rand\_int}(1,i)$\cr
\+&${\it swap}(x[j],x[i])$\cr}
\enddisplay
This three-step method uses $2m$ calls to {\it rand\_int\/}.

After this column originally appeared in {\sl Communications fo the ACM,\/}
sevral readers observed that a slight modification of the above code places a
random $m$-element permutation from the integers in $1\ltdot n$ in
$x[1\ltdot m]$
\begindisplay
\vbox{
\+\bf for $i\leftarrow1$ to $n$ do\cr
\+\quad&$x[i]\leftarrow i$\cr
\+\bf for $i\leftarrow1$ to $m$ do\cr
\+&$j\leftarrow{\it rand\_int}(i,n)$\cr
\+&${\it swap}(x[j],x[i])$\cr}
\enddisplay
This algorithm is easy to code, but it requires $O(n)$ run time and $O(n)$ words
of memory. Floyd's algorithms, which we'll soon see, are more efficient when $n$
is large compared to $m$.

Floyd's random permutation generator is similar to his \alg{F2}. To
compute an $m$-element permutation from $1\ltdot n,$ it first computes an
$(m-1)$-element permutation from $1\ltdot n-1.$ (A recursive version of the
algorithm does not have the variable $j.$) The primary data structure of the
permutation generator, though, is a sequence rather than a set. Here is Floyd's
\alg P:
\begindisplay
\vbox{
\+$S\leftarrow\{\}$\cr
\+\bf for $j\leftarrow n-m+1$ to $n$ do\cr
\+\quad&$t\leftarrow{\it rand\_int}(1,j)$\cr
\+&\bf if $t\not\in S$ then\cr
\+&\quad&prefix $t$ to $S$\cr
\+&\bf else\cr
\+&& insert $j$ in $S$ after $t$\cr}
\enddisplay
Problem~5 shows that \alg P\ is remarkably efficient in terms of the number
of random bits it uses. Problem~6 deals with efficient implementations of the
sequence $S.$

We can get an intuitive feeling for \alg P\ by considering its behavior when
$m=n$, in which case it generates a random permutation of $n$ elements. It
iterates $j$ from 1 to $n$. Before execution of the loop body, $S$ is a random
permutation of the integers in $1\ltdot j-1$. The loop body maintains the
invariant by inserting $j$ into the sequence; $j$ is the first element when
$t=j$, otherwise $j$ is placed after one of the $j-1$ existing elements at
random.

In general, \alg P\ generates every $m$-element permutation of $1\ltdot n$
with equal probability. Floyd's proof of correctness uses the loop invariant
that after $i$ times through the iteration, $j=n-m+i$ and $S$ can be any
permutation of $i$ distinct integers in $1\ltdot j,$ and that there is a single
way that permutation can be generated.

Doug McIlroy found an elegant way to phrase Floyd's proof: there is one and only
one way to produce each permutation, because the algorithm can be run backward.
Suppose, for instance, that $m=5,$ that $n=10,$ and that the final sequence is
\begindisplay
$7\ \;2\ \;9\ \;1\ \;5$
\enddisplay
Because $10$ (the final value of $j$) does not occur in $S$, the previous
sequence must have been
\begindisplay
$2\ \;9\ \;1\ \;5$
\enddisplay
and {\it rand\_int\/} returned $t=7.$ Because $9$ (the relevant value of $j$)
occurs in the $4$-element sequence after $2$, the previous $t$ was $S$.
Problem~4 shows that one can similarly recover the entire sequence of random
values. Because all random sequences are supposedly equally likely, all
permutations are also.

We can now prove the correctness of \alg{F2} by its similarity to \alg P. At
all times, the set $S$ in \alg{F2} contains exactly the elements in the
sequence $S$ in \alg P. Thus each final $m$-element subset of $1\ltdot n$ is
generated by $m!$ random sequences, so all occur equiprobably.

\section Principles

\alg S\ is a pretty good algorithm, but not good enough for Bob Floyd. Not
content with its inefficiency, he developed optimal algorithms for generating
random samples and random permutations. His programs are a model of efficiency,
simplicity, and elegance. Section~13.6 sketches some of the methods that Floyd
uses to achieve such programs.

\section Problems

\vskip-\medskipamount

\prob
How do the various algorithms behave when the {\it rand\_int\/}
procedure is nonrandom? Consider, for instance, generators that always return
0, or that cycle over a range that is much smaller than or much greater than
$m$ or $n$.

\prob
Describe efficient representations for the set $S$ in \alg{F2}.

\prob
\alg S\ and~F2 both use a set $S$. Is a data structure that is
efficient in one algorithm necessarily efficient in the other?

\prob
Complete the proof of correctness of \alg P\ by showing how to compute
from a final permutation the sequence of random integer values that produce it.

\prob
How many random bits does \alg P\ consume? Show that this number is
close to optimal. Perform a similar analysis for \alg{F2}. Can you find
algorithm that are more efficient?

\prob
Describe representations for the sequence $S$ such tat \alg P\ runs
in $O(m)$ expected time or in $O(m\log m)$ worst-case time. Your structures
should use $O(m)$ worst-case space.

\prob
Implement Floyd's algorithms in your favorite programming language.

\prob
\alg{F2} is an iterative version of the recursive \alg{F1}. There are many
general methods for transforming recursive functions to equivalent iterative
programs; one method is often illustrated on a recursive factorial fucntion.
Consider a recursive function with this form
\begindisplay
\vbox{
\+\bf function $A(m)$\cr
\+\quad&\bf if $m = 0$ then\cr
\+&\quad&\bf return $X$\cr
\+&\bf else\cr
\+&&$S\leftarrow A(m-1)$\cr
\+&&\bf return $G(S, m)$\cr}
\enddisplay
\more where $m$ is an integer, $S$ and $X$ have the same type $T,$ and
function~$G$ maps a $T$ and an integer to a $T.$ Show how the function can be
transformded to this iterative form
\begindisplay
\vbox{
\+\bf function $B(m)$\cr
\+\quad&$S\leftarrow X$\cr
\+&\bf for $j\leftarrow1$ to $m$ do\cr
\+&\quad $S\leftarrow G(S,j)$\cr
\+&\bf return $S$\cr}
\enddisplay

\prob
Study other algorithms for generating random samples.

\section Further Reading

Robert~W. Floyd received the ACM Turing Award in 1978. In his Turing lecture on
``The Paradigms of Programming,'' Floyd writes, ``In my own experience of
designing difficult algorithms, I find a certain technique most helpful in
expanding my own capabilities. After solving a challenging problem, I solve it
again from scratch, retracing only the {\it insight\/} of the earlier solution.
I repeat this until the solution is as clear as and direct as I can hope for.
Then I look for a general rule for attacking similiar problems, that
{\it would\/} have led me to approach the given problem in the most efficient
way the first time. Often, such a rule is of permanent value.''

Floyd's key rule in this problem was, in his own words, to ``look for a
mathematical characterization of the solution before you think about an
algorithm to obtain it.'' His key insight dealt with the probability of the
algorithm generating any particular subset. When Floyd calculated the
probabilities of key events in \alg S, he noticed that the denominators were
powers of $N$, while the denominators in the solution were falling factorials.
His algorithms use a simple structure to generate the correct probabilities.
When Floyd finally conceived \alg P, he recalls, ``I knew it was right even
before I proved it.''

Floyd's 1978 Turing lecture was published originally in the August 1979
{\sl Communications of the ACM\/.} It also appears in the {\sl ACM Turing Award
Lectures: The First Twenty Years: 1966--1985\/}, which was published in 1987 by
the ACM Press.
 
\bye
